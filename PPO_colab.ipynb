{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "outputId": "f81d3b1f-5dc7-4a51-f217-cb3a412386b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Cloning into 'PPO-PyTorch'...\n",
            "remote: Enumerating objects: 368, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 368 (delta 91), reused 68 (delta 68), pack-reused 264 (from 2)\u001b[K\n",
            "Receiving objects: 100% (368/368), 12.39 MiB | 22.11 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs\n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mrn6rpJpF8Sc",
        "outputId": "bdf18d3e-7e2f-4058-a2dd-67ee954bfe52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "'./PPO-PyTorch/LICENSE' -> './LICENSE'\n",
            "'./PPO-PyTorch/make_gif.py' -> './make_gif.py'\n",
            "'./PPO-PyTorch/plot_graph.py' -> './plot_graph.py'\n",
            "'./PPO-PyTorch/PPO_colab.ipynb' -> './PPO_colab.ipynb'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png' -> './PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png' -> './PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png' -> './PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png' -> './PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png' -> './PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif' -> './PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif' -> './PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif' -> './PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif' -> './PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth' -> './PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth' -> './PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth' -> './PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/README.md' -> './PPO_preTrained/README.md'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth' -> './PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO.py' -> './PPO.py'\n",
            "'./PPO-PyTorch/README.md' -> './README.md'\n",
            "'./PPO-PyTorch/test.py' -> './test.py'\n",
            "'./PPO-PyTorch/train.py' -> './train.py'\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs\n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-7AbGA2F8Ut",
        "outputId": "a3639bc6-6d99-4d55-aa0e-f7c689dd5254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rbpSQTflGlAr",
        "outputId": "6f1150fa-fd20-4b7b-e09b-c1798d28e400",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "# !pip install roboschool==1.0.7 gym==0.15.4\n",
        "\n",
        "# !pip install box2d-py\n",
        "\n",
        "# !pip install Box2D\n",
        "\n",
        "# !pip install pybullet\n",
        "\n",
        "# !pip install gym[box2d]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "笔记本分为5个主要部分：\n",
        "\n",
        "第一部分：定义演员-评论家网络和PPO算法\n",
        "\n",
        "第二部分：训练PPO算法并保存网络权重和日志文件\n",
        "\n",
        "第三部分：加载（预训练的）网络权重并测试PPO算法\n",
        "\n",
        "第四部分：加载日志文件并绘制图表\n",
        "\n",
        "第五部分：安装Xvfb，加载（预训练的）网络权重，保存用于生成GIF的图像，然后生成GIF\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT6VUBg-F8Zm",
        "outputId": "24751688-2d97-4675-a311-2312a6aaae17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "以下是添加了中文注释的代码：\n",
        "PythonCopy\n",
        "############################### 导入库 ###############################\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "# import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## 设置设备 ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# 设置设备为CPU或CUDA\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"设备设置为：\" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"设备设置为：cpu\")\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO策略网络 ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    \"\"\"\n",
        "    用于存储采样数据的类\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.actions = []  # 存储动作\n",
        "        self.states = []  # 存储状态\n",
        "        self.logprobs = []  # 存储动作的对数概率\n",
        "        self.rewards = []  # 存储奖励\n",
        "        self.state_values = []  # 存储状态价值\n",
        "        self.is_terminals = []  # 存储是否为终止状态的标志\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"\n",
        "        清空缓冲区\n",
        "        \"\"\"\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    演员-评论家网络\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        \"\"\"\n",
        "        初始化函数\n",
        "        :param state_dim: 状态维度\n",
        "        :param action_dim: 动作维度\n",
        "        :param has_continuous_action_space: 是否为连续动作空间\n",
        "        :param action_std_init: 动作标准差初始化值\n",
        "        \"\"\"\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # 演员网络\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        # 评论家网络\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        \"\"\"\n",
        "        设置动作标准差\n",
        "        :param new_action_std: 新的动作标准差\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"警告：在离散动作空间策略上调用 ActorCritic::set_action_std()\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"\n",
        "        前向传播函数（未实现）\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        选择动作\n",
        "        :param state: 当前状态\n",
        "        :return: 动作、动作的对数概率、状态价值\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        state_val = self.critic(state)\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "        \"\"\"\n",
        "        评估函数\n",
        "        :param state: 状态\n",
        "        :param action: 动作\n",
        "        :return: 动作的对数概率、状态价值、分布的熵\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # 对于单动作连续环境\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    \"\"\"\n",
        "    PPO算法类\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "        \"\"\"\n",
        "        初始化函数\n",
        "        :param state_dim: 状态维度\n",
        "        :param action_dim: 动作维度\n",
        "        :param lr_actor: 演员网络学习率\n",
        "        :param lr_critic: 评论家网络学习率\n",
        "        :param gamma: 折扣因子\n",
        "        :param K_epochs: 更新迭代次数\n",
        "        :param eps_clip: PPO的截断参数\n",
        "        :param has_continuous_action_space: 是否为连续动作空间\n",
        "        :param action_std_init: 动作标准差初始化值\n",
        "        \"\"\"\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        \"\"\"\n",
        "        设置动作标准差\n",
        "        :param new_action_std: 新的动作标准差\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"警告：在离散动作空间策略上调用 PPO::set_action_std()\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        \"\"\"\n",
        "        动作标准差衰减\n",
        "        :param action_std_decay_rate: 动作标准差衰减率\n",
        "        :param min_action_std: 动作标准差最小值\n",
        "        \"\"\"\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"将演员输出动作标准差设置为最小值：\", self.action_std)\n",
        "            else:\n",
        "                print(\"将演员输出动作标准差设置为：\", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"警告：在离散动作空间策略上调用 PPO::decay_action_std()\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"\n",
        "        选择动作\n",
        "        :param state: 当前状态\n",
        "        :return: 动作\n",
        "        \"\"\"\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        更新函数\n",
        "        \"\"\"\n",
        "        # 使用蒙特卡洛方法估计回报\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # 归一化奖励\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # 将列表转换为张量\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # 计算优势\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # 优化策略K次\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # 评估旧动作和值\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # 匹配状态值张量的维度与奖励张量\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # 计算比率 (pi_theta / pi_theta_old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # 计算替代损失\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # PPO的最终损失函数\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # 反向传播\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # 将新权重复制到旧策略\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # 清空缓冲区\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        保存模型\n",
        "        :param checkpoint_path: 模型保存路径\n",
        "        \"\"\"\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        加载模型\n",
        "        :param checkpoint_path: 模型加载路径\n",
        "        \"\"\"\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pybullet"
      ],
      "metadata": {
        "id": "I2gAcnTeZwPx",
        "outputId": "47aad569-5c51-4949-be3c-c4c08cb3a662",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Downloading pybullet-3.2.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "069c6638-3716-4c33-e63d-a7626ca1734c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : CartPole-v1\n",
            "current logging run number for CartPole-v1 :  3\n",
            "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_3.csv\n",
            "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training at (GMT) :  2025-02-15 11:40:16\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode : 69 \t\t Timestep : 1600 \t\t Average Reward : 23.14\n",
            "Episode : 133 \t\t Timestep : 3200 \t\t Average Reward : 25.03\n",
            "Episode : 174 \t\t Timestep : 4800 \t\t Average Reward : 38.98\n",
            "Episode : 197 \t\t Timestep : 6400 \t\t Average Reward : 68.83\n",
            "Episode : 212 \t\t Timestep : 8000 \t\t Average Reward : 101.27\n",
            "Episode : 225 \t\t Timestep : 9600 \t\t Average Reward : 125.54\n",
            "Episode : 236 \t\t Timestep : 11200 \t\t Average Reward : 147.82\n",
            "Episode : 243 \t\t Timestep : 12800 \t\t Average Reward : 228.14\n",
            "Episode : 250 \t\t Timestep : 14400 \t\t Average Reward : 233.71\n",
            "Episode : 255 \t\t Timestep : 16000 \t\t Average Reward : 293.4\n",
            "Episode : 261 \t\t Timestep : 17600 \t\t Average Reward : 272.17\n",
            "Episode : 267 \t\t Timestep : 19200 \t\t Average Reward : 281.5\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:23\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 272 \t\t Timestep : 20800 \t\t Average Reward : 310.0\n",
            "Episode : 279 \t\t Timestep : 22400 \t\t Average Reward : 218.0\n",
            "Episode : 285 \t\t Timestep : 24000 \t\t Average Reward : 240.83\n",
            "Episode : 289 \t\t Timestep : 25600 \t\t Average Reward : 384.5\n",
            "Episode : 297 \t\t Timestep : 27200 \t\t Average Reward : 240.0\n",
            "Episode : 301 \t\t Timestep : 28800 \t\t Average Reward : 365.75\n",
            "Episode : 307 \t\t Timestep : 30400 \t\t Average Reward : 271.33\n",
            "Episode : 312 \t\t Timestep : 32000 \t\t Average Reward : 334.2\n",
            "Episode : 318 \t\t Timestep : 33600 \t\t Average Reward : 274.83\n",
            "Episode : 323 \t\t Timestep : 35200 \t\t Average Reward : 318.2\n",
            "Episode : 327 \t\t Timestep : 36800 \t\t Average Reward : 334.75\n",
            "Episode : 332 \t\t Timestep : 38400 \t\t Average Reward : 371.6\n",
            "Episode : 337 \t\t Timestep : 40000 \t\t Average Reward : 284.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:42\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 341 \t\t Timestep : 41600 \t\t Average Reward : 400.0\n",
            "Episode : 346 \t\t Timestep : 43200 \t\t Average Reward : 361.0\n",
            "Episode : 351 \t\t Timestep : 44800 \t\t Average Reward : 311.8\n",
            "Episode : 357 \t\t Timestep : 46400 \t\t Average Reward : 261.83\n",
            "Episode : 363 \t\t Timestep : 48000 \t\t Average Reward : 264.0\n",
            "Episode : 368 \t\t Timestep : 49600 \t\t Average Reward : 323.4\n",
            "Episode : 373 \t\t Timestep : 51200 \t\t Average Reward : 314.2\n",
            "Episode : 377 \t\t Timestep : 52800 \t\t Average Reward : 389.0\n",
            "Episode : 381 \t\t Timestep : 54400 \t\t Average Reward : 400.0\n",
            "Episode : 385 \t\t Timestep : 56000 \t\t Average Reward : 364.25\n",
            "Episode : 392 \t\t Timestep : 57600 \t\t Average Reward : 259.14\n",
            "Episode : 397 \t\t Timestep : 59200 \t\t Average Reward : 309.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:00\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 401 \t\t Timestep : 60800 \t\t Average Reward : 400.0\n",
            "Episode : 407 \t\t Timestep : 62400 \t\t Average Reward : 281.0\n",
            "Episode : 411 \t\t Timestep : 64000 \t\t Average Reward : 333.75\n",
            "Episode : 415 \t\t Timestep : 65600 \t\t Average Reward : 381.5\n",
            "Episode : 419 \t\t Timestep : 67200 \t\t Average Reward : 400.0\n",
            "Episode : 423 \t\t Timestep : 68800 \t\t Average Reward : 400.0\n",
            "Episode : 427 \t\t Timestep : 70400 \t\t Average Reward : 400.0\n",
            "Episode : 431 \t\t Timestep : 72000 \t\t Average Reward : 400.0\n",
            "Episode : 435 \t\t Timestep : 73600 \t\t Average Reward : 400.0\n",
            "Episode : 439 \t\t Timestep : 75200 \t\t Average Reward : 400.0\n",
            "Episode : 444 \t\t Timestep : 76800 \t\t Average Reward : 351.0\n",
            "Episode : 449 \t\t Timestep : 78400 \t\t Average Reward : 367.0\n",
            "Episode : 453 \t\t Timestep : 80000 \t\t Average Reward : 400.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:19\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 457 \t\t Timestep : 81600 \t\t Average Reward : 395.5\n",
            "Episode : 461 \t\t Timestep : 83200 \t\t Average Reward : 382.5\n",
            "Episode : 465 \t\t Timestep : 84800 \t\t Average Reward : 400.0\n",
            "Episode : 469 \t\t Timestep : 86400 \t\t Average Reward : 355.25\n",
            "Episode : 475 \t\t Timestep : 88000 \t\t Average Reward : 280.0\n",
            "Episode : 479 \t\t Timestep : 89600 \t\t Average Reward : 400.0\n",
            "Episode : 484 \t\t Timestep : 91200 \t\t Average Reward : 358.2\n",
            "Episode : 488 \t\t Timestep : 92800 \t\t Average Reward : 375.0\n",
            "Episode : 492 \t\t Timestep : 94400 \t\t Average Reward : 400.0\n",
            "Episode : 497 \t\t Timestep : 96000 \t\t Average Reward : 301.0\n",
            "Episode : 503 \t\t Timestep : 97600 \t\t Average Reward : 299.0\n",
            "Episode : 507 \t\t Timestep : 99200 \t\t Average Reward : 400.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:40\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-02-15 11:40:16\n",
            "Finished training at (GMT) :  2025-02-15 11:41:56\n",
            "Total training time  :  0:01:40\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZWyhkq9Gxm5",
        "outputId": "19a5d790-a501-47b0-ea4c-d3c5438ac471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "<ipython-input-14-0b9b7ec07704>:303: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
            "<ipython-input-14-0b9b7ec07704>:304: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1 \t\t Reward: 239.0\n",
            "Episode: 2 \t\t Reward: 400.0\n",
            "Episode: 3 \t\t Reward: 400.0\n",
            "Episode: 4 \t\t Reward: 400.0\n",
            "Episode: 5 \t\t Reward: 400.0\n",
            "Episode: 6 \t\t Reward: 400.0\n",
            "Episode: 7 \t\t Reward: 400.0\n",
            "Episode: 8 \t\t Reward: 400.0\n",
            "Episode: 9 \t\t Reward: 400.0\n",
            "Episode: 10 \t\t Reward: 202.0\n",
            "============================================================================================\n",
            "average test reward : 364.1\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "bY-E5HGcGxiu",
        "outputId": "4f560245-a628-4df7-e9a8-7cdc8da4da05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_1.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_2.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_3.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YaWPRW9EGxgH",
        "outputId": "9eb6f974-3b76-4fed-cb74-22dde2407d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally.\n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "outputId": "b8c47a4c-a89e-4c07-ba6c-e7532ddd4617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'roboschool'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3c6e16788d01>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mroboschool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'roboschool'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name;\n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoVshl_ZHK7s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20d1bR8xHK5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3V8c8Q9AaKSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "e7JowRQEGGKQ",
        "Z4VJcUT2GlJz"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}